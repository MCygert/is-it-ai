{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.expanduser(\"~/Dev/personal/is_it_ai\"))\n",
    "from utils.data_ops import get_whole_dataset\n",
    "import gc\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import optuna\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train_v2_drcat_02.csv\", sep=',')\n",
    "test = pd.read_csv('data/test_essays.csv')\n",
    "train = train.drop_duplicates(subset=['text'])\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "X = train[\"text\"]\n",
    "y = train['label']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "LOWERCASE = False\n",
    "VOCAB_SIZE = 30522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "raw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if LOWERCASE else [])\n",
    "raw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\n",
    "dataset = Dataset.from_pandas(test[['text']])\n",
    "def train_corp_iter(): \n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000][\"text\"]\n",
    "raw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=raw_tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "tokenized_texts_test = []\n",
    "\n",
    "for text in tqdm(x_test):\n",
    "    tokenized_texts_test.append(tokenizer.tokenize(text))\n",
    "\n",
    "tokenized_texts_train = [] \n",
    "for text in tqdm(x_train):\n",
    "    tokenized_texts_train.append(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(text):\n",
    "    return text\n",
    "vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, analyzer = 'word',\n",
    "    tokenizer = dummy,\n",
    "    preprocessor = dummy,\n",
    "    token_pattern = None, strip_accents='unicode')\n",
    "\n",
    "vectorizer.fit(tokenized_texts_test)\n",
    "\n",
    "# Getting vocab\n",
    "vocab = vectorizer.vocabulary_\n",
    "\n",
    "print(vocab)\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, vocabulary=vocab,\n",
    "                            analyzer = 'word',\n",
    "                            tokenizer = dummy,\n",
    "                            preprocessor = dummy,\n",
    "                            token_pattern = None, strip_accents='unicode'\n",
    "                            )\n",
    "\n",
    "tf_train = vectorizer.fit_transform(tokenized_texts_train)\n",
    "tf_test = vectorizer.transform(tokenized_texts_test)\n",
    "\n",
    "del vectorizer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB(alpha=0.1)\n",
    "sgd_model = SGDClassifier(max_iter=8000, tol=1e-4, loss=\"modified_huber\") \n",
    "p6={'n_iter': 2500,\n",
    "    'verbose': -1,\n",
    "    'objective': 'cross_entropy',\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.01, \n",
    "    'colsample_bytree': 0.78,\n",
    "    'colsample_bynode': 0.8, \n",
    "    'lambda_l1': 4.562963348932286, \n",
    "    'lambda_l2': 2.97485, \n",
    "    'min_data_in_leaf': 115, \n",
    "    'max_depth': 23, \n",
    "    'max_bin': 898}\n",
    "\n",
    "lgb=LGBMClassifier(**p6)\n",
    "# cat=CatBoostClassifier(iterations=2000,\n",
    "#                         verbose=0,\n",
    "#                         l2_leaf_reg=6.6591278779517808,\n",
    "#                         learning_rate=0.1,\n",
    "#                         subsample = 0.4,\n",
    "#                         allow_const_label=True,loss_function = 'CrossEntropy')\n",
    "weights = [0.068,0.311,0.31]\n",
    "\n",
    "ensemble = VotingClassifier(estimators=[('mnb',clf),\n",
    "                                        ('sgd', sgd_model),\n",
    "                                        ('lgb',lgb), \n",
    "                                        # ('cat', cat)\n",
    "                                        ],\n",
    "                            weights=weights, voting='soft', n_jobs=-1)\n",
    "ensemble.fit(tf_train, y_train)\n",
    "gc.collect()\n",
    "final_preds = ensemble.predict_proba(tf_test)[:,1]\n",
    "final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
